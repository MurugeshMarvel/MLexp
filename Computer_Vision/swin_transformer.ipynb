{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    img_size=28, \n",
    "    patch_size=2, \n",
    "    in_chans=1, \n",
    "    num_classes=10, \n",
    "    global_pool='avg',\n",
    "    embed_dim=64, \n",
    "    depths=[2,], \n",
    "    num_heads=[8,], \n",
    "    head_dim=None,\n",
    "    window_size=2, \n",
    "    mlp_ratio=4., \n",
    "    qkv_bias=True,\n",
    "    drop_rate=0.1, \n",
    "    attn_drop_rate=0.1, \n",
    "    drop_path_rate=0.1,\n",
    "    norm_layer=nn.LayerNorm, \n",
    "    ape=False, \n",
    "    patch_norm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.models.swin_transformer:Input for window partition shape - torch.Size([1, 14, 14, 1])\n",
      "DEBUG:src.models.swin_transformer:trying to change shape to 7, 2, 7, 2, 1\n"
     ]
    }
   ],
   "source": [
    "model = SwinTransformer(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(1, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "  (layers): Sequential(\n",
       "    (0): BasicLayer(\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "SwinTransformer                                    --\n",
       "├─PatchEmbed: 1-1                                  --\n",
       "│    └─Conv2d: 2-1                                 320\n",
       "│    └─LayerNorm: 2-2                              128\n",
       "├─Dropout: 1-2                                     --\n",
       "├─Sequential: 1-3                                  --\n",
       "│    └─BasicLayer: 2-3                             --\n",
       "│    │    └─Sequential: 3-1                        100,112\n",
       "├─LayerNorm: 1-4                                   128\n",
       "├─Linear: 1-5                                      650\n",
       "===========================================================================\n",
       "Total params: 101,194\n",
       "Trainable params: 101,194\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = T.rand((1,1,28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 24, 24])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.models.swin_transformer:Input shape - torch.Size([1, 1, 28, 28])\n",
      "DEBUG:src.models.swin_transformer:After patch embedding - torch.Size([1, 196, 64])\n",
      "DEBUG:src.models.swin_transformer:After postitional embed dropout - torch.Size([1, 196, 64])\n",
      "DEBUG:src.models.swin_transformer:Input for Basic Layer - torch.Size([1, 196, 64])\n",
      "DEBUG:src.models.swin_transformer:Input for window partition shape - torch.Size([1, 14, 14, 64])\n",
      "DEBUG:src.models.swin_transformer:trying to change shape to 7, 2, 7, 2, 64\n",
      "DEBUG:src.models.swin_transformer:Input for window partition shape - torch.Size([1, 14, 14, 64])\n",
      "DEBUG:src.models.swin_transformer:trying to change shape to 7, 2, 7, 2, 64\n",
      "DEBUG:src.models.swin_transformer:Output from Block Layer - torch.Size([1, 196, 64])\n",
      "DEBUG:src.models.swin_transformer:After Swin Layers - torch.Size([1, 196, 64])\n",
      "DEBUG:src.models.swin_transformer:After Norm - torch.Size([1, 196, 64])\n",
      "DEBUG:src.models.swin_transformer:After Forward feature - torch.Size([1, 196, 64])\n",
      "/Users/murugesan.vadivel/DEV/MyRepo/MLexp/Computer_Vision/src/models/swin_transformer.py:557: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "a = model.forward(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_cfg(\n",
    "        model_cls: Callable,\n",
    "        variant: str,\n",
    "        pretrained: bool,\n",
    "        pretrained_cfg: Optional[Dict] = None,\n",
    "        model_cfg: Optional[Any] = None,\n",
    "        feature_cfg: Optional[Dict] = None,\n",
    "        pretrained_strict: bool = False,\n",
    "        pretrained_filter_fn: Optional[Callable] = None,\n",
    "        pretrained_custom_load: bool = False,\n",
    "        kwargs_filter: Optional[Tuple[str]] = None,\n",
    "        **kwargs):\n",
    "    \"\"\" Build model with specified default_cfg and optional model_cfg\n",
    "    This helper fn aids in the construction of a model including:\n",
    "      * handling default_cfg and associated pretrained weight loading\n",
    "      * passing through optional model_cfg for models with config based arch spec\n",
    "      * features_only model adaptation\n",
    "      * pruning config / model adaptation\n",
    "    Args:\n",
    "        model_cls (nn.Module): model class\n",
    "        variant (str): model variant name\n",
    "        pretrained (bool): load pretrained weights\n",
    "        pretrained_cfg (dict): model's pretrained weight/task config\n",
    "        model_cfg (Optional[Dict]): model's architecture config\n",
    "        feature_cfg (Optional[Dict]: feature extraction adapter config\n",
    "        pretrained_strict (bool): load pretrained weights strictly\n",
    "        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n",
    "        pretrained_custom_load (bool): use custom load fn, to load numpy or other non PyTorch weights\n",
    "        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n",
    "        **kwargs: model args passed through to model __init__\n",
    "    \"\"\"\n",
    "    pruned = kwargs.pop('pruned', False)\n",
    "    features = False\n",
    "    feature_cfg = feature_cfg or {}\n",
    "\n",
    "    # resolve and update model pretrained config and model kwargs\n",
    "    pretrained_cfg = resolve_pretrained_cfg(variant, pretrained_cfg=pretrained_cfg)\n",
    "    update_pretrained_cfg_and_kwargs(pretrained_cfg, kwargs, kwargs_filter)\n",
    "    pretrained_cfg.setdefault('architecture', variant)\n",
    "\n",
    "    # Setup for feature extraction wrapper done at end of this fn\n",
    "    if kwargs.pop('features_only', False):\n",
    "        features = True\n",
    "        feature_cfg.setdefault('out_indices', (0, 1, 2, 3, 4))\n",
    "        if 'out_indices' in kwargs:\n",
    "            feature_cfg['out_indices'] = kwargs.pop('out_indices')\n",
    "\n",
    "    # Build the model\n",
    "    model = model_cls(**kwargs) if model_cfg is None else model_cls(cfg=model_cfg, **kwargs)\n",
    "    model.pretrained_cfg = pretrained_cfg\n",
    "    model.default_cfg = model.pretrained_cfg  # alias for backwards compat\n",
    "    \n",
    "    if pruned:\n",
    "        model = adapt_model_from_file(model, variant)\n",
    "\n",
    "    # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n",
    "    num_classes_pretrained = 0 if features else getattr(model, 'num_classes', kwargs.get('num_classes', 1000))\n",
    "    if pretrained:\n",
    "        if pretrained_custom_load:\n",
    "            # FIXME improve custom load trigger\n",
    "            load_custom_pretrained(model, pretrained_cfg=pretrained_cfg)\n",
    "        else:\n",
    "            load_pretrained(\n",
    "                model,\n",
    "                pretrained_cfg=pretrained_cfg,\n",
    "                num_classes=num_classes_pretrained,\n",
    "                in_chans=kwargs.get('in_chans', 3),\n",
    "                filter_fn=pretrained_filter_fn,\n",
    "                strict=pretrained_strict)\n",
    "\n",
    "    # Wrap the model in a feature extraction module if enabled\n",
    "    if features:\n",
    "        feature_cls = FeatureListNet\n",
    "        if 'feature_cls' in feature_cfg:\n",
    "            feature_cls = feature_cfg.pop('feature_cls')\n",
    "            if isinstance(feature_cls, str):\n",
    "                feature_cls = feature_cls.lower()\n",
    "                if 'hook' in feature_cls:\n",
    "                    feature_cls = FeatureHookNet\n",
    "                elif feature_cls == 'fx':\n",
    "                    feature_cls = FeatureGraphNet\n",
    "                else:\n",
    "                    assert False, f'Unknown feature class {feature_cls}'\n",
    "        model = feature_cls(model, **feature_cfg)\n",
    "        model.pretrained_cfg = pretrained_cfg_for_features(pretrained_cfg)  # add back default_cfg\n",
    "        model.default_cfg = model.pretrained_cfg  # alias for backwards compat\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a3d88c904243d2c3f246166597f86d1c0a39f3d97496d1fe394945d0c6d436d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
